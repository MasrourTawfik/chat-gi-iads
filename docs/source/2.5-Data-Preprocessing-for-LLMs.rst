Data preprocessing for LLMs
==================================
Data is a collection of discrete values that convey information, describing quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted.

.. figure:: ../Images/DataTypes.jpeg
   :width: 80%
   :align: center
   :alt: Alternative text for the image



Data preprocessing is a crucial step in our pipeline since a bad data preprocessing leads to bad results.It involves a series of steps to prepare the raw text data for training. 
Effective preprocessing contributes to smoother training, faster convergence, and improved model performance. The goal is to standardize the input data to reduce the complexity that the model needs to handle.
Data preprocessing for large language model involves prominent techniques to pre-process its data .

.. figure:: ../Images/Data-Preprocessing-for-LLMs.jpg
   :width: 80%
   :align: center
   :alt: Alternative text for the image

Data cleaning
------------------------
Data cleaning is a fundamental aspect of data pre-processing for training LLMs. This technique involves identifying and rectifying inaccuracies, inconsistencies, and irrelevant elements within the raw text data. 

Data cleaning methods 
------------------------
1.Handling missing values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Missing values can occur when there is no data for some observations or features in a dataset. These gaps in data can lead to inaccurate predictions or a biased model.
To handle missing value ,we have the following techniques:
       1.Data imputation: is the substitution of an estimated value that is as realistic as possible for a missing or problematic data item.
       
       2.Deletion: Where rows or columns with missing values are removed.

2.Noise reduction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Noise in data refers to irrelevant or random information that can distort the true pattern and lead to inaccurate model predictions.

3.Consistency checks
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Consistency checks ensure the data across the dataset adheres to consistent formats, rules, or conventions.Inconsistencies arec caused by data entry erros or system glitches.These inconsistencies can lead to misleading results when training models.

4.Deduplication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Duplicated data can occur for various reasons including data entry errors or system glitches. These duplicates can skew the data distribution and lead to biased model training. By removing duplicates, the dataset becomes more accurate and representative, improving the performance of the LLM.

Text cleaning and normalization
-------------------------------
Removing noise and normalization 
-----------------------------
 Removing noise from text and standardizing it are crucial steps for improving the quality of our textual data and enhancing the performance of the natural language processing tasks.Many techniques are used to fix this issue.

1.Lowercasing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Lowercasing is a text preprocessing step where all letters in the text are converted to lowercase. This step is implemented so that the algorithm does not treat the same words differently in different situations.

